{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService \n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import webbrowser\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "# run browser in headless mode\n",
    "options.add_argument('headless')\n",
    "\n",
    "# instantiate driver \n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all URL'S & save it to text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('R:\\_users\\sunpatil\\scripts\\sb_links.txt', 'w') as f:\n",
    "#     for i in range (0, 2050, 50):\n",
    "#         # load website \n",
    "#         url = 'https://pressagent.envisionconnect.com/results.phtml?agency=sbc&violsortfield=TB_CORE_INSPECTION_VIOL.UPDATE_BY&forceresults=&offset={}&businessname=&businessstreet=&city=&zip=&facilityid=&FTS=&soundslike=&sort=FACILITY_NAME'.format(i)\n",
    "#         # print(url)\n",
    "#         # webpages.append(url)\n",
    "#         # print(webpages)\n",
    "#         f.write(url+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading each url and appending to list to open in webbrowser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('R:/_users/sunpatil/scripts/links.txt', \"r\") as f1:\n",
    "with open('R:\\_users\\sunpatil\\scripts\\sb_links.txt', \"r\") as f1:\n",
    "    links = f1.readlines()\n",
    "    webpages = []\n",
    "    for k in links:\n",
    "        # print(k)\n",
    "        webpages.append(k)\n",
    "        webpages = [l.strip() for l in webpages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excel = openpyxl.Workbook()\n",
    "# print(excel.sheetnames)\n",
    "# sheet = excel.active\n",
    "# sheet.title = 'santa_barbara'\n",
    "# sheet.append(['FacID', 'name', 'add', 'city', 'state', 'postal', 'phone', 'date'])\n",
    "\n",
    "options = webdriver.ChromeOptions() \n",
    "# # run browser in headless mode \n",
    "# options.headless = True \n",
    "# options.add_argument('headless')\n",
    "# instantiate driver \n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "wp = []\n",
    "for j in webpages:\n",
    "    # webbrowser.open_new_tab(j)\n",
    "    wp.append(j)\n",
    "    # get the entire website content\n",
    "    # driver.get(j)\n",
    "    \n",
    "    for k in wp:\n",
    "        page = requests.get(k, verify=False)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        tr = soup.find_all('tr', class_='bodytext')\n",
    "\n",
    "        driver.get(k)\n",
    "\n",
    "    for i in tr:\n",
    "        try:\n",
    "            a = i.find('a', href=True)\n",
    "            # print(a)\n",
    "            name = a.text.replace('amp;  ','')\n",
    "            # print(name)\n",
    "            id = a['href'].split('=',2)[2]\n",
    "            # print(id)\n",
    "            # td = i.find_all('td')[1]\n",
    "            # add = td.text.split()\n",
    "            # add = ' '.join(add)\n",
    "            nm = []\n",
    "            nm.append(id)\n",
    "            nm.append(name)\n",
    "            # nm.append(add)\n",
    "            \n",
    "            # print(nm)\n",
    "\n",
    "            a_tag = driver.find_element(By.LINK_TEXT, name).click()\n",
    "\n",
    "            html_source = driver.page_source\n",
    "            soup1 = BeautifulSoup(html_source, 'html.parser')\n",
    "            # print(soup1)\n",
    "\n",
    "            body = soup1.find('body')\n",
    "            # print(body)\n",
    "\n",
    "            for tag in range(7,10):\n",
    "                try:\n",
    "                    tr1 = body.find_all('tr')[tag]\n",
    "                    tr1 = tr1.find_all('td')[1].text.replace(\"\t\t\t\t\t\t\", \"\").strip()\n",
    "                    # print(tr1)\n",
    "                    nm.append(tr1)\n",
    "                    # print(nm)\n",
    "                except:\n",
    "                    pass\n",
    "              \n",
    "            res = []\n",
    "            for l in nm:\n",
    "                res.append(l.replace(\"\\n\", \"\"))\n",
    "            # print(res)\n",
    "\n",
    "            for s in res:\n",
    "                try: \n",
    "                    res1 = res[3].split(', ')\n",
    "                    res2 = res1[1].split()\n",
    "                    # res.extend(res3)\n",
    "                    # res.extend\n",
    "                    # res3\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "            res.insert(3, res1[0])\n",
    "            res.insert(4, res2[0])\n",
    "            res.insert(5, res2[1])\n",
    "            res.pop(6)\n",
    "\n",
    "            for tag in range(11,12):\n",
    "                try:\n",
    "                    tr1 = body.find_all('tr')[tag]\n",
    "                    tr1 = tr1.find_all('td')[0]\n",
    "                    tr1 = tr1.text.strip()\n",
    "                    res.append(tr1)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # res = [res]  \n",
    "            # col = ['ID', 'Name', 'Address', 'City', 'State', 'Postal', 'Phone', 'Date']\n",
    "            # with open('C:/Users/sunpatil/Desktop/san_luis_ob5.csv', 'w', encoding='utf-8') as f:\n",
    "            #     write = csv.writer(f)\n",
    "            #     write.writerow(col)\n",
    "            #     for row in res:\n",
    "            #         write.writerows([row])\n",
    "            \n",
    "            sheet.append(res)\n",
    "            print(res)\n",
    "            driver.back()\n",
    "        except:\n",
    "            pass\n",
    "excel.save('C:/Users/sunpatil/Desktop/san_luis_ob6.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m td:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     a \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, href\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 33\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamp;  \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# print(name)\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m a[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m3\u001b[39m)[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# excel = openpyxl.Workbook()\n",
    "# # print(excel.sheetnames)\n",
    "# sheet = excel.active\n",
    "# sheet.title = 'santa_barbara'\n",
    "# sheet.append(['ID', 'name', 'add', 'city', 'state', 'postal', 'phone', 'date'])\n",
    "\n",
    "options = webdriver.ChromeOptions() \n",
    "# # run browser in headless mode \n",
    "# options.headless = True \n",
    "# options.add_argument('headless')\n",
    "# instantiate driver \n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "wp = []\n",
    "for j in webpages[0:1]:\n",
    "    # webbrowser.open_new_tab(j)\n",
    "    wp.append(j)\n",
    "    # get the entire website content\n",
    "    # driver.get(j)\n",
    "    \n",
    "    for k in wp:\n",
    "        driver.get(k)\n",
    "        page = requests.get(k, verify=False)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        td = soup.find_all('td', class_='bodyText')\n",
    "        td1 = soup.find_all('td', class_='bodyText')[1::2]\n",
    "        # print(td)\n",
    "        # print(td1)\n",
    "\n",
    "    for i in td:\n",
    "        # try:\n",
    "        a = i.find('a', href=True)\n",
    "        name = a.text.replace('amp;  ','').strip()\n",
    "        # print(name)\n",
    "        id = a['href'].split('=',3)[3].strip()\n",
    "        # print(id)\n",
    "        nm = []\n",
    "        nm.append(id)\n",
    "        nm.append(name)\n",
    "        # print(nm)\n",
    "    \n",
    "    for i in td1:\n",
    "        nm.append(i)\n",
    "        print(nm)\n",
    "        # add = [i.replace('<td class=\"bodyText\" valign=\"top\">','') for i in td1]\n",
    "        # add = list(map(lambda st: str.replace(st, '<td class=\"bodyText\" valign=\"top\">', ''), td1))\n",
    "        # print(str(add))\n",
    "        \n",
    "            \n",
    "            # for l in range (1, 100, 2):\n",
    "            #     td1 = td[l].text.strip()\n",
    "            #     # print(td1)\n",
    "            #     nm = []\n",
    "            #     nm.append(id)\n",
    "            #     nm.append(name)\n",
    "            #     nm.append(td1)\n",
    "            # print(nm)\n",
    "            \n",
    "            # a_tag = driver.find_element(By.LINK_TEXT, name).click()\n",
    "\n",
    "            # html_source = driver.page_source\n",
    "            # soup1 = BeautifulSoup(html_source, 'html.parser')\n",
    "            # # print(soup1)\n",
    "\n",
    "            # body = soup1.find('body')\n",
    "            # # print(body)\n",
    "\n",
    "            # # # for tag in range(3,6):\n",
    "            # # #     try:\n",
    "            # # #         tr = body.find_all('tr')[tag]\n",
    "            # # #         td1 = tr.find_all('td')[1].text.replace(\"                            \", \"\").strip()\n",
    "            # # #         # print(td1)\n",
    "            # # #         nm.append(td1)\n",
    "            # # #     except:\n",
    "            # # #         pass\n",
    "        \n",
    "            # for tag in range(4,5):\n",
    "            #     try:\n",
    "            #         tr = body.find_all('tr')[tag]\n",
    "            #         td2 = tr.find_all('td')[0].text.strip()\n",
    "            #         # print(td2)\n",
    "            #         nm.append(td2)\n",
    "            #     except:\n",
    "            #         pass\n",
    "            \n",
    "            # res = []\n",
    "            # for item in nm:\n",
    "            #     res.append(item.replace('\\n','').replace('\\t','').replace('\\r','').replace('                             ','').replace('             ',''))\n",
    "            #     # print(res)\n",
    "        \n",
    "            # for item in res:\n",
    "            #     try: \n",
    "            #         res1 = res[-1].split(', ')\n",
    "            #         res2 = res1[1].split()\n",
    "            #     except:\n",
    "            #         pass\n",
    "            # # print(res1)\n",
    "            # # print(res2)\n",
    "        \n",
    "            # res.insert(3, res1[0])\n",
    "            # res.insert(4, res2[0])\n",
    "            # res.insert(5, res2[1])\n",
    "            # res.pop(-1)\n",
    "            # # print(res)\n",
    "            \n",
    "            # for tag in range(7,8):\n",
    "            #     try:\n",
    "            #         tr = body.find_all('tr')[tag]\n",
    "            #         td3 = tr.find_all('td')[1].text.strip()\n",
    "            #         # print(td3)\n",
    "            #         res.append(td3)\n",
    "            #     except:\n",
    "            #         pass\n",
    "            # print(res)\n",
    "            # print(nm)\n",
    "        #     driver.back()\n",
    "        #     # link = driver.find_element_by_link_text('Next Page')\n",
    "        #     # link.click()\n",
    "        #     # next_page = driver.find_element(By.LINK_TEXT, 'Next Page').click()\n",
    "        # except:\n",
    "        #     pass\n",
    "    # print(nm)        \n",
    "    # excel.save('C:/Users/sunpatil/Desktop/santa_barbara.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(td1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_element(By.LINK_TEXT, name).click()\n",
    "link = driver.find_element(By.LINK_TEXT, 'Next Page').click()\n",
    "link.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial & error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# webpages = []\n",
    "# for i in range (0, 50, 50):\n",
    "#     # load website \n",
    "#     url = 'https://pressagent.envisionconnect.com/results.phtml?agency=slo&offset={}&businessname=&businessstreet=&city=&zip=&facilityid=&FTS=&soundslike=&sort=PERMIT%20DESC'.format(i)\n",
    "#     # print(url)\n",
    "#     webpages.append(url)\n",
    "    \n",
    "#     for j in webpages:\n",
    "#         driver.get(j)\n",
    "#         # print(driver)\n",
    "#         # get the entire website content \n",
    "#         # driver.get(url)\n",
    "#         page = requests.get(j, verify=False)\n",
    "#         soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "#         tr = soup.find('tr', class_='bodytext')\n",
    "#         a = tr.find_all('a', href=True)[0]\n",
    "#         name = a.text\n",
    "#         id = a['href'].split('=',2)[2]\n",
    "#         a_tag = driver.find_element(By.LINK_TEXT, name).click()\n",
    "#     # print(name)\n",
    "#     # print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excel = openpyxl.Workbook()\n",
    "# print(excel.sheetnames)\n",
    "# sheet = excel.active\n",
    "# sheet.title = 'Health_Insp'\n",
    "# sheet.append(['Name', 'St_add', 'City', 'phone'])\n",
    "# nm = []\n",
    "# nm = body.find_all('tr')[6]\n",
    "# nm = nm.find_all('td')[1].text\n",
    "# nm.append(tr)\n",
    "# print(nm)\n",
    "# for tag in range(6,10):\n",
    "#     tr = body.find_all('tr')[tag]\n",
    "#     tr = tr.find_all('td')[1].text.replace(\"\t\t\t\t\t\t\", \"\").strip()\n",
    "    # tr = td.replace(\"\t\t\t\t\t\t\t\", \"\").strip()#.lstrip().rstrip()\n",
    "    # add = \" \".join(line.strip() for line in add.splitlines())\n",
    "    # print(tr)\n",
    "#     sheet.append([tr])\n",
    "# excel.save('R:/_users/sunpatil/scripts/san_luis_ob1.xlsx')\n",
    "    # nm.append(tr)\n",
    "    # nm.replace(' ','')\n",
    "    # for i in nm:\n",
    "    #     j = i.replace(' ','')\n",
    "    #     # nm.append(j)\n",
    "# print(nm)\n",
    "    # tr1 = tr[tag].text.split()#.strip()\n",
    "    # # tr2 = ''.join(tr1[:])\n",
    "    # print(tr1)\n",
    "# res = []\n",
    "# for k in nm:\n",
    "#     res.append(k.replace(\"\\n\", \"\"))\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Front page output - id, name, address, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel = openpyxl.Workbook()\n",
    "print(excel.sheetnames)\n",
    "sheet = excel.active\n",
    "sheet.title = 'Health_Insp'\n",
    "sheet.append(['FacID', 'Name', 'Address', 'Score'])\n",
    "\n",
    "for j in range (0, 50, 50):\n",
    "    url = 'https://pressagent.envisionconnect.com/results.phtml?agency=slo&offset={}&businessname=&businessstreet=&city=&zip=&facilityid=&FTS=&soundslike=&sort=PERMIT%20DESC'.format(j)\n",
    "    # print(url)\n",
    "    page = requests.get(url, verify=False)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    # tr = soup.find('tr', class_='bodytext')\n",
    "\n",
    "    for i in soup.find_all('tr', class_='bodytext'): \n",
    "        # print(tr)\n",
    "        a = i.find('a', href=True)#[1]\n",
    "        # print(a)\n",
    "        id = a.attrs.get('href', 'Not found').split('=',2)[2]\n",
    "        name = a.text\n",
    "        td = i.find_all('td')[1]\n",
    "        add = td.text.split()\n",
    "        add = ' '.join(add)\n",
    "        # add = td.text.replace(\"\t\t\t\t\t\t\t\", \"\").strip()#.lstrip().rstrip()\n",
    "        # add = \" \".join(line.strip() for line in add.splitlines())\n",
    "        td1 = i.find_all('td')[2]\n",
    "        score = td1.text\n",
    "        print(id)\n",
    "        print(name)\n",
    "        print(add)\n",
    "        print(score)\n",
    "        sheet.append([id, name, add, score])\n",
    "\n",
    "    excel.save('R:/_users/sunpatil/scripts/san_luis_ob.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
